{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages and clone repo\n",
    "!pip install lightning\n",
    "!git clone https://github.com/mhbakalar/phla-prediction.git\n",
    "\n",
    "# Restart runtime after package installation\n",
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd phla-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "data_root = '/content/phla-prediction/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "from lightning.app.components import LightningTrainerMultiNode\n",
    "from lightning.app.storage import Drive\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "import models\n",
    "\n",
    "import models.datasets.phla_binding\n",
    "import models.modules.transformer\n",
    "\n",
    "from models.modules.split_transformer import Transformer\n",
    "\n",
    "import tbdrive\n",
    "\n",
    "class PeptidePrediction(L.LightningWork):\n",
    "    def __init__(self, *args, tb_drive, embedding_dim=128, heads=1, layers=1, save_dir=\"logs\", load_from_checkpoint=False, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.drive = tb_drive\n",
    "        self.cloud_build_config = L.BuildConfig()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.heads = heads\n",
    "        self.layers = layers\n",
    "        self.save_dir = save_dir\n",
    "        self.load_from_checkpoint = load_from_checkpoint\n",
    "\n",
    "        # Build configuration string\n",
    "        self.config = \"heads_{0}_layers_{1}\".format(self.heads,self.layers)\n",
    "\n",
    "    def _next_checkpoint_id(self):\n",
    "        # Find latest version from Drive\n",
    "        vid = 0\n",
    "        version = self.config+\"/version_{0}\".format(vid)\n",
    "        while self.save_dir+\"/lightning_logs/\"+version in self.drive.list(self.save_dir+\"/lightning_logs/\"+self.config):\n",
    "            vid += 1\n",
    "            version = self.config+\"/version_{0}\".format(vid)\n",
    "        \n",
    "        return vid\n",
    "    \n",
    "    def _get_latest_checkpoint(self, version):\n",
    "        ckpt_options = self.drive.list(self.save_dir+\"/lightning_logs/\"+version)\n",
    "        ckpt_options = fnmatch.filter(ckpt_options, '*.ckpt')\n",
    "        ckpt_path = ckpt_options[-1]\n",
    "        self.drive.get(ckpt_path, overwrite=True)   # Overwrite only needed for local execution\n",
    "        return ckpt_path\n",
    "\n",
    "    def run(self):\n",
    "        # Find latest version from Drive\n",
    "        vid = self._next_checkpoint_id()\n",
    "        if(self.load_from_checkpoint):\n",
    "            vid -= 1\n",
    "        version = self.config+\"/version_{0}\".format(vid)\n",
    "        print(\"Version: \", version)\n",
    "\n",
    "        # Configure data\n",
    "        hits_file = 'data/hits_95.txt'\n",
    "        decoys_file = 'data/decoys.txt'\n",
    "        aa_order_file = 'data/amino_acid_ordering.txt'\n",
    "        allele_sequence_file = 'data/alleles_95_variable.txt'\n",
    "\n",
    "        datamodule = models.datasets.phla_binding.DataModule(\n",
    "            hits_file=hits_file,\n",
    "            decoys_file=decoys_file,\n",
    "            aa_order_file=aa_order_file,\n",
    "            allele_sequence_file=allele_sequence_file,\n",
    "            decoy_mul=1,\n",
    "            decoy_pool_mul=1,\n",
    "            train_test_split=0.2,\n",
    "            batch_size=64,\n",
    "            predict_mode=False\n",
    "        )\n",
    "        datamodule.prepare_data()\n",
    "\n",
    "        # Configure the model\n",
    "        model = None\n",
    "        if(self.load_from_checkpoint):\n",
    "            ckpt_path = \"\"\n",
    "            try:\n",
    "                ckpt_path = self._get_latest_checkpoint(version)\n",
    "                print(\"Loading checkpoint path: \", ckpt_path)\n",
    "                model = Transformer.load_from_checkpoint(ckpt_path)\n",
    "            except:\n",
    "                print(\"Could not load checkpoint from path: \", ckpt_path)\n",
    "                print(\"Incrementing version.\")\n",
    "                vid += 1\n",
    "                version = self.config+\"/version_{0}\".format(vid)\n",
    "        \n",
    "        if model == None:\n",
    "            model = Transformer(\n",
    "                peptide_length=12,\n",
    "                allele_length=34,\n",
    "                dropout_rate=0.3,\n",
    "                embedding_dim=self.embedding_dim,\n",
    "                transformer_heads=self.heads,\n",
    "                transformer_layers=self.layers,\n",
    "                learning_rate=1e-4\n",
    "            )\n",
    "\n",
    "        # Create a logger\n",
    "        logger = tbdrive.DriveTensorBoardLogger(\n",
    "            drive=self.drive,\n",
    "            save_dir=self.save_dir,\n",
    "            version=version\n",
    "        )\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(dirpath=logger.log_dir, save_top_k=2, monitor=\"val_loss\")\n",
    "        trainer = L.Trainer(\n",
    "            max_epochs=5,\n",
    "            logger=logger,\n",
    "            callbacks=[checkpoint_callback],\n",
    "            reload_dataloaders_every_n_epochs=1,\n",
    "            accelerator=\"gpu\"\n",
    "        )\n",
    "        trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "        # Manually upload the log state to Drive\n",
    "        try:\n",
    "            print(\"Final upload including checkpoint files...\")\n",
    "            logger._upload_to_storage(logs_only=False)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "config = {'embedding_dim': 200, 'heads': 25, 'layers': 1}\n",
    "\n",
    "component = PeptidePrediction(\n",
    "    tb_drive=Drive(\"lit://hits_95\", component_name=\"pmhc\"),\n",
    "    embedding_dim=config['embedding_dim'],\n",
    "    heads=config['heads'],\n",
    "    layers = config['layers'],\n",
    "    load_from_checkpoint=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d6f6171c878022954ad2c480bec0b37cf777d596adaa7cc6e37da37919f1df6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
